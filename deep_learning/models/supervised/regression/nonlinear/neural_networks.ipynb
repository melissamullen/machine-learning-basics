{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **neural network** is a data processing pipeline composed of layers, each of which performs a transformation on the input data into a new representation, until it closely resembles the desired output.\n",
    "\n",
    "**Gradient descent** is the optimization technique that powers neural networks:\n",
    "\n",
    "* A prediction is made on a training example.\n",
    "\n",
    "* The **loss function** compares the prediction to the actual target, producing a **loss value**: a measure of how well the model's prediction matches what was expected.\n",
    "\n",
    "* A **backward pass** computes the gradient of the loss with respect to the model parameters, which describes how the loss varies as you move the model's coefficients in different directions. You can use this gradient to move the coefficients all at once in a direction that decreases the loss.\n",
    "\n",
    "* The **optimizer** uses the loss value to update the model's weights via the **Backpropagation algorithm**. The optimizer might update the model weights using: learning_rate * gradient (the **learning rate** represents the \"speed\" of the gradient descent process)\n",
    "\n",
    "* This process is usually performed in randomly selected batches, so it is called **mini-batch stochastic gradient descent**. Running the process on all the data at once would be more accurate but more expensive.\n",
    "\n",
    "Tensorflow is capable of autodifferentiation using GradientDescent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Autodifferentiation with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 11:44:53.832101: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-22 11:45:01.777630: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(0.)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = 2 * x + 3\n",
    "grad_of_y_wrt_x = tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Implementation of a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "        w_shape = (input_size, output_size)\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "\n",
    "        b_shape = (output_size,)\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "learning_rate = 1e-3\n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        w.assign_sub(g * learning_rate) # equivalent of -= for tensorflow variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent (One Step)\n",
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Make predictions for all examples in the batch\n",
    "        predictions = model(images_batch)\n",
    "\n",
    "        # Compute the losses for all examples in the batch\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "\n",
    "        # Get the average loss over all examples in the batch\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "\n",
    "    # Compute the gradient of the loss with respect to the model weights\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "\n",
    "    # Update the weights in such a way that will decrease the loss (e.g. learning_rate * gradient)\n",
    "    update_weights(gradients, model.weights)\n",
    "\n",
    "    # Return the average loss over all examples in this batch\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Generator\n",
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit (Training)\n",
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "\n",
    "    # For each epoch  (pass through the whole training dataset)\n",
    "    for epoch_counter in range(epochs):\n",
    "\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "\n",
    "        # For each randomly selected batch of training examples\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "\n",
    "            # Make predictions (forward pass) and compute the loss\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "\n",
    "            if batch_counter % 100 == 0:\n",
    "\n",
    "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a Neural Network model using the Naive Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model definition\n",
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28*28, output_size=512, activation=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "])\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss at batch 0: 5.08\n",
      "loss at batch 100: 2.25\n",
      "loss at batch 200: 2.23\n",
      "loss at batch 300: 2.11\n",
      "loss at batch 400: 2.27\n",
      "Epoch 1\n",
      "loss at batch 0: 1.96\n",
      "loss at batch 100: 1.90\n",
      "loss at batch 200: 1.85\n",
      "loss at batch 300: 1.76\n",
      "loss at batch 400: 1.88\n",
      "Epoch 2\n",
      "loss at batch 0: 1.64\n",
      "loss at batch 100: 1.60\n",
      "loss at batch 200: 1.53\n",
      "loss at batch 300: 1.46\n",
      "loss at batch 400: 1.55\n",
      "Epoch 3\n",
      "loss at batch 0: 1.38\n",
      "loss at batch 100: 1.37\n",
      "loss at batch 200: 1.27\n",
      "loss at batch 300: 1.24\n",
      "loss at batch 400: 1.30\n",
      "Epoch 4\n",
      "loss at batch 0: 1.17\n",
      "loss at batch 100: 1.19\n",
      "loss at batch 200: 1.07\n",
      "loss at batch 300: 1.07\n",
      "loss at batch 400: 1.12\n",
      "Epoch 5\n",
      "loss at batch 0: 1.02\n",
      "loss at batch 100: 1.05\n",
      "loss at batch 200: 0.92\n",
      "loss at batch 300: 0.95\n",
      "loss at batch 400: 0.99\n",
      "Epoch 6\n",
      "loss at batch 0: 0.91\n",
      "loss at batch 100: 0.94\n",
      "loss at batch 200: 0.82\n",
      "loss at batch 300: 0.85\n",
      "loss at batch 400: 0.90\n",
      "Epoch 7\n",
      "loss at batch 0: 0.82\n",
      "loss at batch 100: 0.85\n",
      "loss at batch 200: 0.73\n",
      "loss at batch 300: 0.78\n",
      "loss at batch 400: 0.83\n",
      "Epoch 8\n",
      "loss at batch 0: 0.75\n",
      "loss at batch 100: 0.78\n",
      "loss at batch 200: 0.67\n",
      "loss at batch 300: 0.72\n",
      "loss at batch 400: 0.78\n",
      "Epoch 9\n",
      "loss at batch 0: 0.70\n",
      "loss at batch 100: 0.72\n",
      "loss at batch 200: 0.62\n",
      "loss at batch 300: 0.67\n",
      "loss at batch 400: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Train the example model on the MNIST image dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of the example model\n",
    "import numpy as np\n",
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy()\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "print(f\"accuracy: {matches.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a Neural Network model using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model definition\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.2611 - accuracy: 0.9242\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1064 - accuracy: 0.9691\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0704 - accuracy: 0.9792\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0508 - accuracy: 0.9850\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.0383 - accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9a0cdc250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the example model on the MNIST image dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of the example model\n",
    "import numpy as np\n",
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy()\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "print(f\"accuracy: {matches.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean squared error (MSE) is a loss function commonly used for regression.\n",
    "- Mean absolute error (MAE) is a common metric used for regression.\n",
    "- When features in the input data have value in different ranges, each feature should be scaled independently as a preprocessing step.\n",
    "- When there is little data available:\n",
    "    - Using K-fold validation is a great way to reliably evaluate a model.\n",
    "    - It is prefereable to use a small model with few intermediate layers (typically only one or two) in order to avoid severe overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting house prices: A regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Predict a continuous value instead of a discrete variable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "# train_images.shape == (404, 13) we have 404 training examples, each with 13 numerical features (e.g. crime rate)\n",
    "# test_images.shape == (102, 13) we have 102 test examples, each with 13 numerical features (e.g. crime rate)\n",
    "\n",
    "# Prepare the data (normalize)\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "train_data -= mean\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std\n",
    "\n",
    "# It would be problematic to feed into a neural network values that all take wildly different ranges. \n",
    "# The model might be able to automatically adapt but it would make learning more difficult. \n",
    "# A common best practice is to normalize the data — that is, to ensure that all values are centered around 0 and have a standard deviation of 1.\n",
    "# normalization = (x - mean) / std\n",
    "# Note that the quantities used for normalizing the test data are computed using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1) # no activation -> linear layer (this is the typical setup for scalar regression) (applying an activation function would constrain the range the output can take)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"mse\", # mean squared error (the square of the difference between the predictions and the targets) (this is a widely used loss function for regression problems)\n",
    "        metrics=[\"mae\"] # mean absolute error (the absolute value of the difference between the predictions and the targets)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataset is small, the validation accuracy can change a lot depending on which examples we choose to use for validation and which we choose for training.\n",
    "# The best practice in such situations is to use K-fold cross-validation.\n",
    "# It consists of splitting the available data into K partitions (typically K=4 or 5),\n",
    "# instantiating K identical models, and training each one on K-1 partitions while evaluating on the remaining partitions.\n",
    "# The validation score for the model used is then the average of the K validation scores obtained.\n",
    "\n",
    "# K-fold cross-validation\n",
    "import numpy as np\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for fold in range(k):\n",
    "\n",
    "    print(f\"processing fold #{fold}\")\n",
    "\n",
    "    # Select the validation data\n",
    "    val_data = train_data[num_val_samples * fold : num_val_samples * (fold + 1)]\n",
    "    val_targets = train_targets[num_val_samples * fold : num_val_samples * (fold + 1)]\n",
    "\n",
    "    # Prepare the training data\n",
    "    partial_train_data = np.concatenate([\n",
    "        train_data[:num_val_samples * fold],\n",
    "        train_data[num_val_samples * (fold + 1):]\n",
    "    ], axis=0)\n",
    "    partial_train_targets = np.concatenate([\n",
    "        train_targets[:num_val_samples * fold],\n",
    "        train_targets[num_val_samples * (fold + 1):]\n",
    "    ], axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    model.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation data\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)\n",
    "\n",
    "print(f\"all_scores: {all_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
